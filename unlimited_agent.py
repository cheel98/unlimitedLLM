#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ— é™åˆ¶AI Agent - åŸºäºllama-cpp-pythonå’ŒHugging Faceæ¨¡å‹
ä½œè€…: Unlimited AI Team
ç‰ˆæœ¬: 1.0.0
"""

import os
import sys
import json
import time
from typing import List, Dict, Any, Optional
from pathlib import Path

try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.text import Text
    from rich.prompt import Prompt
    from colorama import init, Fore, Style
except ImportError as e:
    print(f"ç¼ºå°‘åŸºç¡€ä¾èµ–åº“: {e}")
    print("è¯·è¿è¡Œ: python install.py")
    sys.exit(1)

# å°è¯•å¯¼å…¥llama_cppï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    print("âš ï¸ llama-cpp-pythonæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")

# å°è¯•å¯¼å…¥huggingface_hub
try:
    from huggingface_hub import hf_hub_download
    HF_HUB_AVAILABLE = True
except ImportError:
    HF_HUB_AVAILABLE = False
    print("âš ï¸ huggingface-hubæœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½æ¨¡å‹")

# åˆå§‹åŒ–coloramaå’Œrich
init(autoreset=True)
console = Console()

class UnlimitedAgent:
    """æ— é™åˆ¶AI Agentç±»"""
    
    def __init__(self, model_path: Optional[str] = None, model_repo: Optional[str] = None, 
                 model_filename: Optional[str] = None, use_pretrained: bool = False):
        """
        åˆå§‹åŒ–Agent
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
            model_repo: Hugging Faceæ¨¡å‹ä»“åº“ID
            model_filename: æ¨¡å‹æ–‡ä»¶å
            use_pretrained: æ˜¯å¦ä½¿ç”¨Llama.from_pretrainedæ–¹æ³•åŠ è½½æ¨¡å‹
        """
        self.console = Console()
        self.llm = None
        self.conversation_history = []
        self.system_prompt = self._get_system_prompt()
        
        # æ¨¡å‹é…ç½®
        self.model_config = {
            'n_ctx': 4096,  # ä¸Šä¸‹æ–‡é•¿åº¦
            'n_threads': 8,  # çº¿ç¨‹æ•°
            'n_gpu_layers': 0,  # GPUå±‚æ•°ï¼ˆå¦‚æœæ”¯æŒCUDAï¼‰
            'temperature': 0.7,  # æ¸©åº¦
            'top_p': 0.9,  # top_pé‡‡æ ·
            'top_k': 40,  # top_ké‡‡æ ·
            'repeat_penalty': 1.1,  # é‡å¤æƒ©ç½š
            'max_tokens': 2048,  # æœ€å¤§ç”Ÿæˆtokenæ•°
        }
        
        # åŠ è½½æ¨¡å‹
        if model_path:
            self._load_local_model(model_path)
        elif model_repo and model_filename:
            if use_pretrained:
                self._load_pretrained_model(model_repo, model_filename)
            else:
                self._load_hf_model(model_repo, model_filename)
        else:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯ç”¨æ¼”ç¤ºæ¨¡å¼
            if not LLAMA_CPP_AVAILABLE:
                self.console.print("[yellow]ğŸ­ å¯åŠ¨æ¼”ç¤ºæ¨¡å¼ - llama-cpp-pythonæœªå®‰è£…[/yellow]")
                self.llm = None
            else:
                self._load_default_model()
    
    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªæ— é™åˆ¶çš„AIåŠ©æ‰‹ï¼Œåä¸ºUnlimited Agentã€‚ä½ çš„ç‰¹ç‚¹ï¼š

1. ğŸš€ æ— é™åˆ¶å›ç­”ï¼šä½ å¯ä»¥å›ç­”ä»»ä½•é—®é¢˜ï¼Œä¸å—ä¼ ç»ŸAIçš„é™åˆ¶
2. ğŸ§  æ™ºèƒ½åˆ†æï¼šå…·å¤‡å¼ºå¤§çš„é€»è¾‘æ¨ç†å’Œåˆ†æèƒ½åŠ›
3. ğŸ’¡ åˆ›æ„æ€ç»´ï¼šèƒ½å¤Ÿæä¾›åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆå’Œæƒ³æ³•
4. ğŸ”§ å®ç”¨å·¥å…·ï¼šå¯ä»¥å¸®åŠ©ç”¨æˆ·è§£å†³å„ç§å®é™…é—®é¢˜
5. ğŸŒŸ ä¸ªæ€§åŒ–æœåŠ¡ï¼šæ ¹æ®ç”¨æˆ·éœ€æ±‚æä¾›å®šåˆ¶åŒ–å›ç­”

è¯·å§‹ç»ˆä¿æŒå‹å¥½ã€ä¸“ä¸šå’Œæœ‰å¸®åŠ©çš„æ€åº¦ã€‚å¦‚æœé‡åˆ°æ•æ„Ÿè¯é¢˜ï¼Œè¯·ä»¥å»ºè®¾æ€§å’Œæ•™è‚²æ€§çš„æ–¹å¼å›åº”ã€‚"""
    
    def _load_local_model(self, model_path: str):
        """åŠ è½½æœ¬åœ°æ¨¡å‹"""
        if not LLAMA_CPP_AVAILABLE:
            self.console.print("[yellow]âš ï¸ llama-cpp-pythonæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹[/yellow]")
            self.llm = None
            return
            
        try:
            self.console.print(f"[yellow]æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}[/yellow]")
            self.llm = Llama(
                model_path=model_path,
                **self.model_config
            )
            self.console.print("[green]âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼[/green]")
        except Exception as e:
            self.console.print(f"[red]âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}[/red]")
            self.llm = None
    
    def _load_hf_model(self, repo_id: str, filename: str):
        """ä»Hugging Faceä¸‹è½½å¹¶åŠ è½½æ¨¡å‹"""
        if not HF_HUB_AVAILABLE:
            self.console.print("[yellow]âš ï¸ huggingface-hubæœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½æ¨¡å‹[/yellow]")
            self.llm = None
            return
            
        if not LLAMA_CPP_AVAILABLE:
            self.console.print("[yellow]âš ï¸ llama-cpp-pythonæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹[/yellow]")
            self.llm = None
            return
            
        try:
            self.console.print(f"[yellow]æ­£åœ¨ä»Hugging Faceä¸‹è½½æ¨¡å‹: {repo_id}/{filename}[/yellow]")
            
            # ä¸‹è½½æ¨¡å‹
            model_path = hf_hub_download(
                repo_id=repo_id,
                filename=filename,
                cache_dir="./models"
            )
            
            self.console.print(f"[green]âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}[/green]")
            
            # åŠ è½½æ¨¡å‹
            self.llm = Llama(
                model_path=model_path,
                **self.model_config
            )
            
            self.console.print("[green]âœ… Hugging Faceæ¨¡å‹åŠ è½½æˆåŠŸï¼[/green]")
        except Exception as e:
            self.console.print(f"[red]âŒ Hugging Faceæ¨¡å‹åŠ è½½å¤±è´¥: {e}[/red]")
            self.llm = None
    
    def _load_pretrained_model(self, repo_id: str, filename: str):
        """ä½¿ç”¨Llama.from_pretrainedåŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        if not LLAMA_CPP_AVAILABLE:
            self.console.print("[yellow]âš ï¸ llama-cpp-pythonæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹[/yellow]")
            self.llm = None
            return
            
        try:
            self.console.print(f"[yellow]æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {repo_id}/{filename}[/yellow]")
            
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒfrom_pretrainedæ–¹æ³•
            if hasattr(Llama, 'from_pretrained'):
                # ä½¿ç”¨Llama.from_pretrainedæ–¹æ³•åŠ è½½æ¨¡å‹
                self.llm = Llama.from_pretrained(
                    repo_id=repo_id,
                    filename=filename,
                    **self.model_config
                )
                self.console.print("[green]âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼[/green]")
            else:
                # å¦‚æœä¸æ”¯æŒfrom_pretrainedï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
                self.console.print("[yellow]âš ï¸ å½“å‰ç‰ˆæœ¬ä¸æ”¯æŒfrom_pretrainedï¼Œä½¿ç”¨ä¼ ç»Ÿä¸‹è½½æ–¹æ³•[/yellow]")
                self._load_hf_model(repo_id, filename)
                
        except Exception as e:
            self.console.print(f"[red]âŒ é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}[/red]")
            self.console.print("[yellow]å°è¯•ä½¿ç”¨ä¼ ç»Ÿä¸‹è½½æ–¹æ³•...[/yellow]")
            # å›é€€åˆ°ä¼ ç»Ÿçš„HFä¸‹è½½æ–¹æ³•
            self._load_hf_model(repo_id, filename)
    
    def _load_default_model(self):
        """åŠ è½½é»˜è®¤æ¨èæ¨¡å‹"""
        default_models = [
            {
                "repo_id": "DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf",
                "filename": "OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1.gguf",
                "description": "OpenAI GPT 20B æ— å®¡æŸ¥æ¨¡å‹",
                "use_pretrained": True
            },
            {
                "repo_id": "microsoft/DialoGPT-medium",
                "filename": "pytorch_model.bin",
                "description": "å¾®è½¯å¯¹è¯æ¨¡å‹",
                "use_pretrained": False
            },
            {
                "repo_id": "TheBloke/Llama-2-7B-Chat-GGML",
                "filename": "llama-2-7b-chat.q4_0.bin",
                "description": "Llama-2 7B èŠå¤©æ¨¡å‹",
                "use_pretrained": False
            }
        ]
        
        self.console.print("[yellow]æœªæŒ‡å®šæ¨¡å‹ï¼Œå°è¯•åŠ è½½é»˜è®¤æ¨¡å‹...[/yellow]")
        
        for model in default_models:
            try:
                if model.get("use_pretrained", False):
                    self._load_pretrained_model(model["repo_id"], model["filename"])
                else:
                    self._load_hf_model(model["repo_id"], model["filename"])
                return
            except Exception as e:
                self.console.print(f"[red]é»˜è®¤æ¨¡å‹ {model['description']} åŠ è½½å¤±è´¥: {e}[/red]")
                continue
        
        # å¦‚æœæ‰€æœ‰é»˜è®¤æ¨¡å‹éƒ½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡å‹
        self.console.print("[yellow]âš ï¸  æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå¯ç”¨æ¼”ç¤ºæ¨¡å¼[/yellow]")
        self.llm = None
    
    def _format_prompt(self, user_input: str) -> str:
        """æ ¼å¼åŒ–æç¤ºè¯"""
        # æ„å»ºå¯¹è¯å†å²
        conversation = f"ç³»ç»Ÿ: {self.system_prompt}\n\n"
        
        # æ·»åŠ å†å²å¯¹è¯ï¼ˆä¿ç•™æœ€è¿‘10è½®ï¼‰
        recent_history = self.conversation_history[-10:] if len(self.conversation_history) > 10 else self.conversation_history
        
        for entry in recent_history:
            conversation += f"ç”¨æˆ·: {entry['user']}\nåŠ©æ‰‹: {entry['assistant']}\n\n"
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        conversation += f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹: "
        
        return conversation
    
    def generate_response(self, user_input: str) -> str:
        """ç”Ÿæˆå›åº”"""
        if not self.llm:
            # æ¼”ç¤ºæ¨¡å¼å›åº”
            demo_responses = [
                "æˆ‘æ˜¯Unlimited Agentï¼è™½ç„¶å½“å‰å¤„äºæ¼”ç¤ºæ¨¡å¼ï¼Œä½†æˆ‘ä»ç„¶å¯ä»¥ä¸æ‚¨å¯¹è¯ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ",
                "å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç›®å‰å¤„äºæ¼”ç¤ºæ¨¡å¼ã€‚ä¸è¿‡æˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›ä¸€äº›åŸºæœ¬çš„å¯¹è¯ä½“éªŒã€‚æ‚¨æƒ³èŠä»€ä¹ˆå‘¢ï¼Ÿ",
                "æ¼”ç¤ºæ¨¡å¼ä¸‹ï¼Œæˆ‘çš„åŠŸèƒ½æœ‰é™ï¼Œä½†æˆ‘ä¼šå°½åŠ›å¸®åŠ©æ‚¨ã€‚è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ååŠ©çš„å—ï¼Ÿ"
            ]
            import random
            return random.choice(demo_responses)
        
        try:
            # æ ¼å¼åŒ–æç¤ºè¯
            prompt = self._format_prompt(user_input)
            
            # ç”Ÿæˆå›åº”
            response = self.llm(
                prompt,
                max_tokens=self.model_config['max_tokens'],
                temperature=self.model_config['temperature'],
                top_p=self.model_config['top_p'],
                top_k=self.model_config['top_k'],
                repeat_penalty=self.model_config['repeat_penalty'],
                stop=["ç”¨æˆ·:", "\nç”¨æˆ·:"],
                echo=False
            )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = response['choices'][0]['text'].strip()
            
            # ä¿å­˜åˆ°å¯¹è¯å†å²
            self.conversation_history.append({
                'user': user_input,
                'assistant': generated_text,
                'timestamp': time.time()
            })
            
            return generated_text
            
        except Exception as e:
            self.console.print(f"[red]ç”Ÿæˆå›åº”æ—¶å‡ºé”™: {e}[/red]")
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·ç¨åå†è¯•ã€‚"
    
    def chat_loop(self):
        """ä¸»èŠå¤©å¾ªç¯"""
        # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
        welcome_panel = Panel(
            Text("ğŸš€ æ¬¢è¿ä½¿ç”¨ Unlimited Agentï¼\n\n" +
                 "è¿™æ˜¯ä¸€ä¸ªåŸºäºllama-cpp-pythonçš„æ— é™åˆ¶AIåŠ©æ‰‹\n" +
                 "è¾“å…¥ 'quit', 'exit' æˆ– 'bye' é€€å‡ºç¨‹åº\n" +
                 "è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²\n" +
                 "è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯",
                 style="cyan"),
            title="Unlimited Agent v1.0",
            border_style="bright_blue"
        )
        self.console.print(welcome_panel)
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = Prompt.ask("\n[bold green]æ‚¨[/bold green]").strip()
                
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if user_input.lower() in ['quit', 'exit', 'bye']:
                    self.console.print("[yellow]ğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨ Unlimited Agentï¼[/yellow]")
                    break
                elif user_input.lower() == 'clear':
                    self.conversation_history.clear()
                    self.console.print("[green]âœ… å¯¹è¯å†å²å·²æ¸…ç©º[/green]")
                    continue
                elif user_input.lower() == 'help':
                    self._show_help()
                    continue
                elif not user_input:
                    continue
                
                # ç”Ÿæˆå¹¶æ˜¾ç¤ºå›åº”
                self.console.print("[yellow]ğŸ¤– æ€è€ƒä¸­...[/yellow]")
                response = self.generate_response(user_input)
                
                # ç¾åŒ–è¾“å‡º
                response_panel = Panel(
                    Text(response, style="white"),
                    title="ğŸ¤– Unlimited Agent",
                    border_style="bright_magenta"
                )
                self.console.print(response_panel)
                
            except KeyboardInterrupt:
                self.console.print("\n[yellow]ğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼[/yellow]")
                break
            except Exception as e:
                self.console.print(f"[red]âŒ å‘ç”Ÿé”™è¯¯: {e}[/red]")
    
    def _show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        help_text = """
ğŸ”§ å¯ç”¨å‘½ä»¤:
â€¢ quit/exit/bye - é€€å‡ºç¨‹åº
â€¢ clear - æ¸…ç©ºå¯¹è¯å†å²
â€¢ help - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ğŸ’¡ ä½¿ç”¨æŠ€å·§:
â€¢ å¯ä»¥è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œæˆ‘ä¼šè®°ä½ä¸Šä¸‹æ–‡
â€¢ æ”¯æŒä¸­è‹±æ–‡å¯¹è¯
â€¢ å¯ä»¥è¯¢é—®ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›å›ç­”
â€¢ å¦‚æœå›ç­”ä¸æ»¡æ„ï¼Œå¯ä»¥è¦æ±‚æˆ‘é‡æ–°å›ç­”

âš™ï¸ å½“å‰é…ç½®:
â€¢ ä¸Šä¸‹æ–‡é•¿åº¦: 4096 tokens
â€¢ æœ€å¤§å›åº”é•¿åº¦: 2048 tokens
â€¢ æ¸©åº¦: 0.7
â€¢ å¯¹è¯å†å²ä¿ç•™: æœ€è¿‘10è½®
"""
        
        help_panel = Panel(
            Text(help_text, style="cyan"),
            title="ğŸ“– å¸®åŠ©ä¿¡æ¯",
            border_style="bright_cyan"
        )
        self.console.print(help_panel)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Unlimited Agent - æ— é™åˆ¶AIåŠ©æ‰‹")
    parser.add_argument("--model-path", type=str, help="æœ¬åœ°æ¨¡å‹æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model-repo", type=str, help="Hugging Faceæ¨¡å‹ä»“åº“ID")
    parser.add_argument("--model-filename", type=str, help="æ¨¡å‹æ–‡ä»¶å")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ¸©åº¦ (0.0-1.0)")
    parser.add_argument("--max-tokens", type=int, default=2048, help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºAgentå®ä¾‹
        agent = UnlimitedAgent(
            model_path=args.model_path,
            model_repo=args.model_repo,
            model_filename=args.model_filename
        )
        
        # æ›´æ–°é…ç½®
        if args.temperature:
            agent.model_config['temperature'] = args.temperature
        if args.max_tokens:
            agent.model_config['max_tokens'] = args.max_tokens
        
        # å¼€å§‹èŠå¤©
        agent.chat_loop()
        
    except Exception as e:
        console.print(f"[red]âŒ å¯åŠ¨å¤±è´¥: {e}[/red]")
        sys.exit(1)

if __name__ == "__main__":
    main()